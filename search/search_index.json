{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>"},{"location":"#semeval-2024-task-1-semantic-textual-relatedness-for-african-and-asian-languages","title":"SemEval-2024 Task 1: Semantic Textual Relatedness for African and Asian Languages","text":"<p>A shared task on automatically detecting the degree of semantic relatedness between pairs of sentences. New textual datasets will be provided for Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Moroccan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu.</p> <p>Part of the 18th International Workshop on Semantic Evaluation</p> <p>Contact organisers at semrel-semeval-organisers@googlegroups.com</p> <p>Development Phase Website</p> <p>Evaluation Phase Website</p> <p>GitHub Page</p> <p>Data</p> <p>Follow us on Twitter</p> <p>Join Task Slack Channel</p> <p>Join Google Group</p> <p></p>"},{"location":"#task-results","title":"Task Results","text":"<p>The shared task has officially ended, and we have released the results. You can access the results at the following links:     <li>Track A Results</li> <li>Track B Results</li> <li>Track C Results</li></p>"},{"location":"#motivation","title":"Motivation","text":"<p>The semantic relatedness of two language units has long been considered fundamental to understanding meaning (Halliday and Hassan 1976, Miller and Charles 1991), and automatically determining relatedness has many applications such as evaluating sentence representation methods, question answering, and summarization (Abdalla et al. 2023).</p> <p>Two sentences are considered semantically similar when they have a paraphrasal or entailment relation. On the other hand, relatedness is a much broader concept that accounts for all the commonalities between two sentences: whether they are on the same topic, express the same view, originate from the same time period, one elaborates on (or follows from) the other, etc. For instance, for the following sentence pairs:</p> <p> <p></p> <p></p> <p>Most people will agree that the sentences in pair 1 are more related than the sentences in pair 2. </p> <p>Much of past NLP work is on semantic similarity, and largely focused on English. In this shared task, we cover the following languages: Afrikaans, Algerian Arabic, Amharic, English, Hausa, Hindi, Indonesian, Kinyarwanda, Marathi, Morrocan Arabic, Modern Standard Arabic, Punjabi, Spanish, and Telugu.</p>"},{"location":"#task-overview","title":"Task Overview","text":"<p>Data</p> <p>Each instance in the training, development, and test sets is a sentence pair. The instance is labeled with a score representing the degree of semantic textual relatedness between the two sentences. The scores can range from 0 (maximally unrelated) to 1 (maximally related). These gold label scores have been determined through manual annotation. Specifically, a comparative annotation approach was used to avoid known limitations of traditional rating scale annotation methods. This comparative annotation process (which avoids several biases of traditional rating scales) led to a high reliability of the final relatedness rankings. Further details about the task, the method of data annotation, how STR is different from semantic textual similarity, applications of semantic textual relatedness, etc. can be found in this paper. </p> <p> The Semantic Textual Relatedness Shared Task 1</p> <p>The task consists of predicting semantic textual relatedness (STR) of sentence pairs. Participants will rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) in the 14 different languages. All sentence pairs will have manually determined relatedness scores between 0 (completely unrelated) and 1 (maximally related). Each team can provide submissions for one, two or all of the tracks shown below:  </p> <p>Track A: Supervised</p> <p>Participants are to submit systems that have been trained using the labeled training datasets provided. Participating teams are allowed to use any publicly available datasets (e.g., other relatedness and similarity datasets or datasets in any other languages). However, they must report additional data they used, and ideally report how impactful each resource was on the final results. </p> <p>Track B: Unsupervised</p> <p>Participants are to submit systems that have been developed without the use of any labeled datasets pertaining to semantic relatedness or semantic similarity between units of text more than two words long in any language. The use of unigram or bigram relatedness datasets (from any language) is permitted.  </p> <p>Track C: Cross-lingual</p> <p>Participants are to submit systems that have been developed without the use of any labeled semantic similarity or semantic relatedness datasets in the target language and with the use of labeled dataset(s) from at least one other language.  Note: Using labeled data from another track is mandatory for a submission to this track. </p> <p>Deciding which track a submission should go to</p> <ul> <li> If a submission uses labeled data in the target language: submit to Track A.</li> <li> If a submission does not use labeled data in the target language but uses labeled data from another language: submit to Track C. </li> <li> If a submission does not use labeled data in any language: submit to Track B.</li> </ul> <p>Note</p> <p>** Here \u2018labeled data\u2019 refers to labeled datasets pertaining to semantic relatedness or semantic similarity between units of text more than two words long. </p>"},{"location":"#evaluation","title":"Evaluation","text":"<p>The official evaluation metric for this task is the Spearman rank correlation coefficient, which captures how well the system-predicted rankings of test instances align with human judgments. You can find the evaluation script for this shared task on our Github page.</p>"},{"location":"#important-dates","title":"Important Dates","text":"Description Deadline Training Data Ready 18 September 2023 Evaluation Start 20 January 2024 Evaluation End 31 January 2024 System Description Paper Due 19 February 2024 Notification to authors 1 April 2024 Camera ready due 22 April 2024 SemEval workshop 2024 June 16\u201321, 2024 (co-located with NAACL2024) <p>All deadlines are 23:59 UTC-12 (\"anywhere on Earth\").</p>"},{"location":"#dataset","title":"Dataset","text":"<p>The data for this shared task is now available here.</p>"},{"location":"#faqs","title":"FAQs","text":""},{"location":"#can-i-use-llms-for-tracks-b-and-c","title":"Can I use LLMs for Tracks B and C?","text":"<p>For tasks B and C, the use of pre-trained language models (PLMs) is permitted, as long as they have not been explicitly trained on any form of text similarity data or objective. </p> <p>Therefore, it is acceptable to use PLMs trained on unlabeled text, whether under a masked objective (such as BERT) or a causal objective (e.g., GPT-2). If the PLM is then further fine-tuned with text similarity data, whether through instruct-tuning (e.g., BLOOMZ), classification, or a similarity objective (like SBERT), then it is not allowed anymore.</p> <p>To ensure fairness, the use of an opaque model, where the training data origins are unclear, such as using the ChatGPT, is also prohibited for tasks B and C.</p> <p>For Track C (cross-lingual), one can use some amount of labeled data from a language other than the target language. I.e., using opaque English models on non-English languages in Track C is fine. </p>"},{"location":"#do-i-have-to-participate-in-all-languages-for-a-given-track","title":"Do I have to participate in all languages for a given track?","text":"<p>No you can participate in one or more languages.</p>"},{"location":"#how-will-you-verify-my-submitted-model","title":"How will you verify my submitted model?","text":"<p>To be included in the final team rankings of our shared task, it is mandatory for participants to submit a system description paper describing their approaches and methodologies in detail therefore ensuring scientific integrity.</p>"},{"location":"#references","title":"References","text":"<p>Shima Asaadi, Saif Mohammad, Svetlana Kiritchenko. 2019. Big BiRD: A Large, Fine-Grained, Bigram Relatedness Dataset for Examining Semantic Composition. Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.</p> <p>M. A. K. Halliday and R. Hasan. 1976. Cohesion in English. London: Longman.</p> <p>George A Miller and Walter G Charles. 1991. Contextual Correlates of Semantic Similarity. Language and Cognitive Processes, 6(1):1\u201328</p> <p>Mohamed Abdalla, Krishnapriya Vishnubhotla, and Saif Mohammad. 2023. What Makes Sentences Semantically Related? A Textual Relatedness Dataset and Empirical Study. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 782\u2013796, Dubrovnik, Croatia. Association for Computational Linguistics.</p>"},{"location":"Resources/","title":"Resources","text":""},{"location":"Resources/#resources-and-faqs","title":"Resources and FAQs","text":"<p>We provide some useful information about SemEval shared task. </p> <ol> <li> <p>SemEval 2024 Shared Tasks</p> </li> <li> <p>Frequently Asked Questions about SemEval</p> </li> <li> <p>Paper Submission Requirements</p> </li> <li> <p>Guidelines for Writing Papers</p> </li> <li> <p>Paper style files</p> </li> <li> <p>Paper submission link (TBD)</p> </li> </ol>"},{"location":"Terms/","title":"Terms","text":"<li>By submitting results to this competition, you consent to the public release of your scores on this website and at the SemEval-2024 workshop and in the associated proceedings, at the task organizers' discretion. Scores may include but are not limited to, automatic and manual quantitative judgments, qualitative judgments, and such other metrics as the task organizers see fit. You accept that the ultimate decision of metric choice and score value is that of the task organizers.</li> <li>You further agree that the task organizers are under no obligation to release scores and that scores may be withheld if it is the task organizers' judgment that the submission was incomplete, erroneous, deceptive, or violated the letter or spirit of the competition's rules. The inclusion of a submission's scores is not an endorsement of a team or individual's submission, system, or science.</li> <li>A participant can be involved in exactly one team (no more). If there are reasons why it makes sense for you to be on more than one team, then email us before the evaluation period begins. In special circumstances, this may be allowed.</li> <li>Each team must create and use exactly one CodaLab account.</li> <li>The team constitution (members of a team) cannot be changed after the evaluation period has begun.</li> <li>During the development period:\u00a0</li> <ul> <li>Each team can submit as many as 999 submissions.</li> <li>You will only be able to see the results of your submission and not others.\u00a0Leaderboard is disabled</li> <li>You will be able to see any warnings and errors for each of your submissions.</li> </ul> <li>During the evaluation period:</li> <ul> <li>Each team can submit only 3 submissions. However, only the final submission will be considered as the official submission to the competition.</li> <li>You will be able to see any warnings and errors for each of your submissions.</li> </ul> <li>Once the competition is over, we will release the gold labels and you will be able to determine the results of various system variants you may have developed. We encourage you to report results on all of your systems (or system variants) in the system-description paper. However, we will ask you to clearly indicate the result of your official submission.</li> <li>We will make the final submissions of the teams public at some point after the evaluation period.</li> <li>The organizers and their affiliated institutions make no warranties regarding the datasets provided, including but not limited to being correct or complete. They cannot be held liable for providing access to the datasets or the usage of the datasets.</li> <li>Each task participant will be assigned another team\u2019s system description papers for review. The papers will thus be peer-reviewed.</li> <li>The dataset should only be used for scientific or research purposes. Any other use is explicitly prohibited.</li> <li>The datasets must not be redistributed or shared in part or full with any third party. Redirect interested parties to this website.</li>"},{"location":"datasets/","title":"Dataset Statistics","text":"Languages \u00a0 Train \u00a0 Val \u00a0 Test \u00a0 Hausa - - - Yoruba - - - Igbo - - Nigerian-Pidgin - - Amharic - - - Algerian Arabic - - - Swahili - - Kinyarwanda - - - Mozabique Portuguess - - - Twi - - - Sestswana - - - Isuzulu - - - Tigrinya - - -"},{"location":"index33/","title":"Index33","text":""},{"location":"index33/#afrisenti-semeval-sentiment-analysis-for-african-languages","title":"AfriSenti-SemEval: Sentiment Analysis for African Languages","text":"<p>Part of The 17th International Workshop on Semantic Evaluation</p> <p>Contact organizers: afrisenti-semeval-organizers@googlegroups.com</p> <p>To communicate with the organizers,\u00a0join the</p> <ul> <li>Mailing Group: https://groups.google.com/g/afrisenti-semeval</li> <li>Slack Channel: AfriSenti</li> </ul> <p>Update (05 February):</p> <ul> <li>The release of official team rankings is now: on February 7th, 2023.</li> </ul> <p>Update (17 January):</p> <ul> <li>The official ranking for the shared task is Weighted F1</li> <li>Evaluation starts: January 20th, 2023</li> <li>Release of dev set gold labels : January 20th, 2023</li> <li>Evaluation ends: January 31st, 2023</li> <li>Team registration form submission deadline: February 3rd, 2023 (we will share the form soon, with more details)</li> <li>System Description Paper deadline: February 20, 2023</li> <li>Release of official team rankings: February 5th, 2023</li> <li>Release of test data gold labels: February 10th, 2023</li> <li>System Description Paper deadline: February 20, 2023</li> </ul> <p>Update (31 October):</p> <ul> <li>Xitsonga (ts) is now in Task A</li> <li>Multilingual (Task B) dataset has been updated to include Xitsonga</li> </ul> <p>Update (26 October):</p> <ul> <li>We released datasets for</li> <ul> <li>Task A: Twi - twi</li> <li>Task C: Xitsonga - ts</li> </ul> <li>We updated the final dataset for Task B - Multilingual (consisting of all released datasets for Task A)</li> </ul> <p>Update (15 October):</p> <ul> <li>We released datasets for</li> <ul> <li>Task A: Kinyarwanda - kr</li> <li>Task C: Oromo - or</li> <li>Task C: Tigrinya - tg</li> </ul> </ul> <p>Update (8 October):</p> <ul> <li>Due to inquiries, we included a \"How to Participate\" tab with step-by-step instructions.</li> <li>We provide a Starter Kit on our GitHub Repo.\u00a0</li> <li>We released sentiment\u00a0lexicons for\u00a0some languages in our GitHub repo.</li> <li>We now have the best paper award, in addition to the best system prizes.</li> </ul> <p>Update (4 October):</p> <ul> <li>We released an updated dataset (Version 2.0). Disregard the previous version and download the current.</li> <li>We added Mozambican Portuguese and will soon release other languages after the annotation</li> </ul> <p>Motivation</p> <p>Due to the widespread use of the Internet and social media platforms, most languages are becoming digitally available. This allows for various artificial intelligence (AI) applications that enable tasks such as sentiment analysis, machine translation and hateful content detection. According to UNESCO (2003), 30% of all living languages, around 2,058, are African languages. However, most of these languages do not have curated datasets for developing such AI applications. Recently, various individual and funded initiatives, such as the Lacuna Fund, have set out to reverse this trend and create such datasets for African languages. However, research is required to determine both the suitability of current natural language processing (NLP) techniques and the development of novel techniques to maximize the applications of such datasets.</p> <p>There has been a growing interest in sentiment analysis which applies to many domains, including public health, commerce/business, art and literature, social sciences, neuroscience, and psychology (Mohammad, Saif M, 2022). Previous shared tasks on sentiment analysis include Mohammad, Saif M et al., (2018), Nakov et al., (2016), Pontiki et al., Ghosh et al., (2015), (2014), and so on. However, none of these tasks included African languages. Though  Mohammad, Saif, et al. (2018) included standard Arabic, we focus on Arabic dialects from African countries: Algerian Arabic and Tunisian Arabizi. We believe SemEval is the right venue, due to its popularity and widespread acceptance, to carry out shared tasks for African languages to strengthen their further development.</p> <p>In this shared task, we have covered 17 African languages, Hausa, Yoruba, Igbo, Nigerian Pidgin from Nigeria, Amharic, Tigrinya, and Oromo from Ethiopia, Swahili from Kenya and Tanzania, Algerian Arabic dialect from Algeria, Kinyarwanda from Rwanda, Twi from Ghana, Mozambique Portuguese from Mozambique \u00a0and Moroccan Arabic/Darija from Morocco.</p>"},{"location":"index33/#task-overview","title":"Task Overview","text":"<p>The AfriSenti-SemEval Shared Task 12 is based on a collection of Twitter datasets in 14 African languages for sentiment classification. It consists of three sub-tasks. Participants can select one or more sub-tasks depending on their preference. In each sub-task also, the participant may wish to participate in any number of languages as so wished.</p> <p>Task A: Monolingual Sentiment Classification</p> <p>Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet conveys both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. This sub-task has 15 tracks:</p> <p>Note: You are free to select one or more tracks in this sub-task.</p> <ul> <li>Track 1: Hausa\u00a0</li> <li>Track 2: Yoruba</li> <li>Track 3: Igbo</li> <li>Track 4: Nigerian_Pidgin</li> <li>Track 5: Amharic</li> <li>Track 6: Algerian Arabic</li> <li>Track 7: Moroccan Arabic/Darija,</li> <li>Track 8: Swahili</li> <li>Track 9: Kinyarwanda</li> <li>Track 10: Twi</li> <li>Track 11: Mozambican Portuguese</li> <li>Track 12: Xitsonga\u00a0(Mozambique Dialect)</li> <li>Track 13: Setswana (data to be released soon)</li> <li>Track 14: isiZulu (data to be released soon)</li> <li>Track 15: Xitsonga (South-African Dialect, to be released soon)</li> </ul> <p>Note: Tweets in each language are code-mix. Read our NaijaSenti paper for more information.</p> <p>Task B: Multilingual Sentiment Classification</p> <p>Given combined training data from Task-A (Track 1 to 12), determine the polarity of a tweet in the target language (positive, negative, or neutral). This sub-task has only one track with 12 languages (Hausa, Yoruba, Igbo, Nigerian_Pidgin, Amharic, Algerian Arabic, Moroccan Arabic/Darija, Swahili, Kinyarwanda, Twi, Mozambican Portuguese, and Xitsonga(Mozambique Dialect)):</p> <ul> <li>Track 16: 12 languages in Task A</li> </ul> <p>Task C: Zero-Shot Sentiment Classification</p> <p>Given unlabelled tweets in two African languages (Tigrinya and Oromo), leverage any or all of the available training datasets (in Task:A ) to determine the sentiment of a tweet in the two target languages. This task has two (2) tracks.</p> <p>Note: You are free to select one or more tracks in this sub-task.</p> <ul> <li>Track 17: Zero-Shot on Tigrinya</li> <li>Track 18: Zero-Shot on Oromo</li> </ul>"},{"location":"index33/#dataset-examples","title":"Dataset Examples","text":"<p>The dataset involves tweets labelled with three sentiment classes (positive, negative, neutral) in 14 African languages. Each tweet is annotated by three annotators following the annotation guidelines in (Mohammad, Saif M, 2016). We use a form of a majority vote to determine the sentiment of the tweet. See more in our paper (Muhammad et al., 2022, Yimam et al., 2020). Below is a sample dataset for the 4 Nigerian languages (Muhammad et al., 2022):</p> <p> </p> <p></p> <p>The datasets are available on the Participate tab.</p> <p> </p>"},{"location":"index33/#why-participate","title":"Starter Kit?","text":"<p>We provide a Starter Kit on our GitHub Repo that can be used to crearte a baseline system.</p>"},{"location":"index33/#why-participate","title":"Why Participate?","text":"<ul> <li>Promote NLP research involving African languages,</li> <li>Opportunity to write a system-description paper that describes their system, resources used, results, and analysis.</li> <li>Stand a chance to win an award.</li> <li>Opportunity to network with renowned experts in the AI and NLP community.</li> </ul> <p>Resources on Paper Submission</p> <ul> <li>Paper Submission Requirements</li> <li>Guidelines for Writing Papers</li> <li>Paper Style Files</li> <li>Paper Submission Site (TBD)</li> </ul>"},{"location":"index33/#communication","title":"Previous Shared Tasks","text":"<ul> <li>Shared tasks in English:\u00a0SemEval-2017,\u00a0SemEval-2016,\u00a0SemEval-2015,\u00a0SemEval-2014,\u00a0SemEval-2013.</li> <li>Shared tasks in Spanish:\u00a0TASS-2017,\u00a0TASS-2016,\u00a0TASS-2015,\u00a0TASS-2014,\u00a0TASS-2013,\u00a0TASS-2012.</li> </ul>"},{"location":"index_update/","title":"Index update","text":"<p>Motivation</p> <p>Due to the widespread use of the Internet and social media platforms, most languages are becoming digitally available. This allows for various artificial intelligence (AI) applications that enable tasks such as sentiment analysis, machine translation and hateful content detection. According to UNESCO (2003), 30% of all living languages, around 2,058, are African languages. However, most of these languages do not have curated datasets for developing such AI applications. Recently, various individual and funded initiatives, such as the Lacuna Fund, have set out to reverse     this trend and create such datasets for African languages. However, research is required to determine both the suitability of current natural language processing (NLP) techniques and the development of novel techniques to maximize the applications of such datasets.</p> <pre><code>&lt;p&gt;There has been a growing interest in sentiment analysis which applies to many domains, including public health, commerce/business, art and literature, social sciences, neuroscience,\nand psychology (&lt;cite&gt;Mohammad, Saif M, 2022&lt;/cite&gt;). Previous shared tasks on sentiment analysis include &lt;cite&gt;Mohammad, Saif M et al., (2018), Nakov et al., (2016), Pontiki et al., Ghosh et al., (2015), (2014), and so on &lt;/cite&gt; . However, none of these tasks included African languages. Though &lt;cite&gt; Mohammad, Saif, et al. (2018)&lt;/cite&gt; included standard Arabic, we focus on Arabic dialects from African countries: &lt;cite&gt;Algerian Arabic&lt;/cite&gt; and &lt;cite&gt;Tunisian Arabizi&lt;/cite&gt;. We believe SemEval is the right venue, due to its popularity and widespread acceptance, to carry out shared tasks for African languages to strengthen their further development. &lt;/p&gt;\n&lt;p&gt;In this shared task, we have covered 16 African languages, &lt;a href=\"https://en.wikipedia.org/wiki/Hausa_language\"&gt;Hausa&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Yoruba_language\"&gt;Yoruba&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Igbo_language\"&gt;Igbo&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Nigerian_Pidgin\"&gt;Nigerian Pigdin&lt;/a&gt; from Nigeria, &lt;a href=\"https://en.wikipedia.org/wiki/Amharic, and Oromo\"&gt;Amharic&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Tigrinya_language\"&gt;Tigrinya&lt;/a&gt; and &lt;a href=\"https://en.wikipedia.org/wiki/Oromo_language\"&gt;Oromo&lt;/a&gt; from Ethiopia, &lt;a href=\"https://en.wikipedia.org/wiki/Swahili_language\"&gt;Swahili&lt;/a&gt; from Kenya and Tanzania,  &lt;a href=\"https://en.wikipedia.org/wiki/Algerian_Arabic\"&gt;Algerian Arabic&lt;/a&gt; dialect from Algeria, &lt;a href=\"https://en.wikipedia.org/wiki/Kinyarwanda\"&gt;Kinyarwanda&lt;/a&gt; from Rwanda, &lt;a href=\"https://en.wikipedia.org/wiki/Twi\"&gt;Twi&lt;/a&gt; from Ghana, &lt;a href=\"https://www.google.com/search?client=safari&amp;amp;rls=en&amp;amp;q=Mozabique+portuguess&amp;amp;ie=UTF-8&amp;amp;oe=UTF-8\"&gt;Mozambique Portuguese&lt;/a&gt; from Mozambique and &lt;a href=\"https://en.wikipedia.org/wiki/Zulu_language\"&gt;isiZulu&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Tswana_language\"&gt;Setswana&lt;/a&gt;, &lt;a href=\"https://en.wikipedia.org/wiki/Tsonga_language\"&gt;Xitsonga&lt;/a&gt; from South Africa and &lt;a href=\"https://en.wikipedia.org/wiki/Moroccan_Arabic\"&gt;Moroccan Arabic/Darija&lt;/a&gt; from Morocco.&lt;/p&gt;\n</code></pre>"},{"location":"index_update/#task-overview","title":"Task Overview","text":"<p>The AfriSenti-SemEval Shared Task 12 is based on a collection of Twitter datasets in 16 African languages for sentiment classification. It consists of three sub-tasks. Participants can select one or more tasks depending on their preference.</p> <p>Task A: Monolingual Sentiment Classification</p> <p>Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet  For messages conveying both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. This sub-task has 13 tracks:</p> <ul> <li>Track 1: Hausa</li> <li>Track 2: Yoruba</li> <li>Track 3: Igbo</li> <li>Track 4: Nigerian_Pidgin</li> <li>Track 5: Amharic</li> <li>Track 6: Algerian Arabic</li> <li>Track 7: Moroccan Arabic/Darija,</li> <li>Track 8: Swahili </li> <li>Track 9: Kinyarwanda (data to be released soon)</li> <li>Track 10: Twi (data to be released soon)</li> <li>Track 11: Mozambican Portuguese (data to be released soon)</li> <li>Track 12: Setswana (data to be released soon)</li> <li>Track 13: isiZulu (data to be released soon)</li> </ul> <p>Task B: Multilingual Sentiment Classification</p> <p>Given a combined training data from 10 African languages, determine the polarity of a tweet in the target language (positive, negative, or neutral). This sub-task has only one track (13 languages):</p> <ul> <li>Track 14: All languages in Task A</li> </ul> <p>Task C: Zero-Shot Sentiment Classification</p> <p>Given unlabeled tweets in three African languages (Tigrinya, Xitsonga and Oromo), leverage any or all of the available training datasets in Subtasks 1 and 2 to determine the sentiment of a tweet in the three target languages is positive, negative, or neutral. This task has three (3) tracks. This task starts on the 1st of October, 2022.</p> <ul> <li>Track 15: Zero-Shot on Tigrinya (dev and test sets to be released)</li> <li>Track 16: Zero-Shot on Xitsonga (dev and test sets to be released)</li> <li>Track 17: Zero-Shot on Oromo (dev and test sets to be released)</li> </ul>"},{"location":"index_update/#dataset-examples","title":"Dataset Examples","text":"<p>The dataset involves tweets labeled with three sentiment classes (positive, negative, neutral) in 14 African languages. Each tweet is annotated by three annotators following the annotation guidelines in (Mohammad, Saif M, 2016). We use a form of majority vote to determine the sentiment of the tweet. See more in our paper (Muhammad et al., 2022, Yimam et al., 2020). Below is a sample dataset for the 4 Nigerian languges (Muhammad et al., 2022):</p> <p></p> <p>The datasets are available on the competition website.</p>"},{"location":"index_update/#why-participate","title":"Why Participate ?Resources on Paper Submission","text":"<ul> <li>Promote NLP research involving African languages,</li> <li>Opportunity to write a system-description paper that describes their system, resources used, results, and analysis.</li> <li>Stand a chance to win award.</li> <li>Opportunity to network with renowned experts in the AI and NLP community.</li> </ul> <ul> <li>Paper Submission Requirements</li> <li>Guidelines for Writing Papers</li> <li>Paper Style Files</li> <li>Paper Submission Site (TBD)</li> </ul>"},{"location":"index_update/#communication","title":"Previous Shared Tasks","text":"<ol> <li>Shared tasks in English:SemEval-2017,SemEval-2016,SemEval-2015,SemEval-2014,SemEval-2013</li> <li>Shared tasks in Spanish TASS-2017,TASS-2016,TASS-2015,TASS-2014,TASS-2013,TASS-2012.</li> </ol>"},{"location":"organizer/","title":"Organizers","text":""},{"location":"organizer/#task-organisers-and-contact","title":"Task Organisers and Contact","text":"<p>If you need additional information about a specific dataset, see below.</p> <ul> <li> <p>For questions about the Afrikaans dataset, please contact Christine de Kock.</p> </li> <li> <p>For questions about the Algerian Arabic and Modern Standard datasets, please contact Nedjma Ousidhoum and Meriem Beloucif.</p> </li> <li> <p>For questions about the Amharic dataset, please contact Seid Muhie Yimam.</p> </li> <li> <p>For questions about the Moroccan Arabic dataset, please contact Oumaima Hourrane.</p> </li> <li> <p>For questions about the English dataset, please contact Krishnapriya Vishnubhotla, Mohamed Abdalla, and Saif M. Mohammad.</p> </li> <li> <p>For questions about the Hausa and Kinyarwanda datasets, please contact Shamsuddeen Hassan Muhammad, Idris Abdulmumin, and Ibrahim Said Ahmad.</p> </li> <li> <p>For questions about the Hindi, Marathi, and Telugu datasets, please contact Nirmal Surange and Manish Shrivastava.</p> </li> <li> <p>For questions about the Indonesian dataset, please contact Alham Fikri Aji.</p> </li> <li> <p>For questions about the Punjabi dataset, please contact Sanchit Ahuja.</p> </li> <li> <p>For questions about the Spanish dataset, please contact Vladimir Araujo and Thamar Solorio.</p> </li> </ul> <p>Below is the list of the task organisers.</p> Organizers Affiliation Contact Nedjma Ousidhoum Cardiff University  \u00a0OusidhoumN@cardiff.ac.uk Shamsuddeen Hassan Muhammad Bayero University, Kano, Masakhane \u00a0shmuhammad.csc@buk.edu.ng Mohamed Abdalla Institute for Better Health, Canada \u00a0msa@cs.toronto.edu Krishnapriya Vishnubhotla University of Toronto \u00a0vkpriya@cs.toronto.edu Vladimir Araujo  Pontificia Universidad Cat\u00f3lica de Chile and KU Leuven \u00a0vgaraujov@uc.cl Idris Abdulmumin Ahmadu Bello University Zaria, Masakhane \u00a0iabdulmumin@abu.edu.ng Meriem Beloucif Uppsala University \u00a0meriem.beloucif@lingfil.uu.se Seid Muhie Yimam Universit\u00e4t Hamburg, Hamburg; Masakhane\u00a0 \u00a0 \u00a0seid.muhie.yimam@uni-hamburg.de Nirmal Surange IIIT Hyderabad \u00a0nirmal.surange@research.iiit.ac.in Christine de Kock The University of Melbourne \u00a0christinedekock11@gmail.com Sanchit Ahuja BITS Pilani \u00a0sanchitahuja205@gmail.com Oumaima Hourrane Independent Researcher \u00a0oumaima.hourrane@gmail.com Manish Shrivastava IIIT Hyderabad \u00a0m.shrivastava@iiit.ac.in Ibrahim Said Ahmad Institute For Experiential AI, Northeastern University \u00a0isab7070@gmail.com Alham Fikri Aji MBZUAI \u00a0Alham.Fikri@mbzuai.ac.ae Thamar Solorio MBZUAI \u00a0thamar.solorio@gmail.com Saif M. Mohammad National Research Council, Canada \u00a0uvgotsaif@gmail.com"},{"location":"participation/","title":"Participation","text":"How To Participate"},{"location":"prizes/","title":"AfriSenti SemEval Prizes","text":"<p>AfriSenti-SemEval competition has a prize and will be awarded to best performing team in each of the three sub-tasks(A, B and C). We also have three leagues for the award:</p> <ul> <li> <p>African League: To encourage African participation, this league is for team with at least one African.</p> </li> <li> <p>Students League: This league is dedicated Masters and Undergraduate students only..</p> </li> <li> <p>Worldwide League: Be a participant from any country.</p> </li> </ul> <p>The prize ($500) will be awarded to the best system from each league. For example, for sub-task A, three awards will be given from the three leagues. So, 9 prizes will be awarded to the total number of winners since we have 3 sub-tasks.</p>"},{"location":"prizes/#competition-terms-and-conditions","title":"Competition Terms and Conditions","text":"<p>The prizes in this competition are to be distributed by the organizers of AfriSenti-SemEval 2023 (\"The Organizers\") and are guided under the following terms and regulations:</p>"},{"location":"prizes/#how-to-enter","title":"How to enter","text":"<ul> <li>Entries received after the stated closing date will not be accepted.</li> <li>The competition is free to enter.</li> <li>The Organizers will not accept responsibility for entries that are lost, mislaid, damaged or delayed in transit, regardless of cause, including, for example, as a result of any postal failure, equipment failure, technical malfunction, systems, satellite, network, server, computer hardware or software failure of any kind.</li> <li>By submitting an entry, you are agreeing to be bound by these Terms and Conditions. If you have any questions, please contact afrisenti-semeval-organizers@googlegroups.com</li> <li>The Organizers reserves the right to refuse entry, or refuse to award the prize to anyone in breach of these terms and conditions.</li> </ul>"},{"location":"prizes/#eligibility","title":"Eligibility","text":"<ul> <li>Only teams that submit system description paper are eligible.</li> <li>Unless otherwise stated, the competitions are open to all except members of the organizing team.</li> <li>Only one entry per team is will be considered.</li> <li>In entering, you confirm that you are eligible to do so and eligible to claim any prize you may win. The Organizers may require you to provide proof that you are eligible to enter the competition.</li> <li>The Organizers reserves all rights to disqualify you if your conduct is contrary to the spirit or intention of the prize.</li> </ul>"},{"location":"prizes/#the-competition","title":"The competition","text":"<ul> <li>For each sub-task, a winner will be chosen by averaging the performances of all the languages in the sub-task. For example, for sub-task-A, the winner will be chosen by averaging the performance of all the languages in the sub-task. </li> <li>The winner will receive prize of $500.</li> <li>The winner will be notified by email (using details provided at registration) within 7 days of being chosen and must provide a means to claim their prize. If a winner does not respond to The Organizers within 14 days of being notified, then the winner\u2019s prize will be forfeited and The Organizers will be entitled to select another winner in accordance with the process described above.</li> <li>The prize is non-exchangeable and non-transferable. The Organizers reserve the right to replace the prize with an alternative prize of equal or higher value if circumstances beyond The Organizers' control makes it necessary to do so.</li> <li>The decision of The Organizers regarding any aspect of the prize is final and binding and no correspondence will be entered into about it.</li> <li>The Organizers reserve the right to hold void, cancel, suspend, or amend the promotion where it becomes necessary to do so.</li> </ul>"},{"location":"prizes/#limitation-of-liability","title":"Limitation of liability","text":"<ul> <li>Insofar as is permitted by law, The Organizers or agents will not in any circumstances be responsible or liable to compensate the winner or accept any liability for any loss as a result of taking up the prize except where it is caused by the negligence of The Organizers or agents. Your statutory rights are not affected.</li> </ul>"},{"location":"prizes/#data-protection-and-publicity","title":"Data protection and publicity","text":"<ul> <li>The Organizers are committed to protecting and respecting your privacy and will only use your personal information in accordance with these Terms and Conditions.</li> <li>By entering, you agree that any personal information provided by you with your entry may be held and used by The Organizers or its agents to administer the competition.</li> </ul>"},{"location":"prizes/#governing-law","title":"Governing law","text":"<ul> <li> <p>All our prizes in this competition will be governed by Nigerian law and entrants to the competition submit to the jurisdiction of the Nigerian courts.</p> </li> <li> <p>The Organizers reserves the right to update these Terms and Conditions from time to time and any updated version will be effective as soon as it is published on the website.</p> </li> </ul>"},{"location":"results/","title":"Results","text":""},{"location":"results/#system-description-paper","title":"System Description paper","text":"<p>We are excited to announce that we will be hosting a workshop on writing system description papers. Participants are encouraged to submit a system description paper, regardless of their ranking. </p> <p>During the workshop, we will explain how to write a system description paper and participants will be able to ask questions. The session will be held on the 9th of February, 2023. The details will be sent by email to all the participants.</p> <p>The workshop will be delivered by Nedjma Ousidhoum and Saif Mohammad</p>"},{"location":"results/#track-1-hausa","title":"Track 1: Hausa","text":""},{"location":"results/#track-2-yoruba","title":"Track 2: Yoruba","text":""},{"location":"results/#track-3-igbo","title":"Track 3: Igbo","text":""},{"location":"results/#track-4-nigerian_pidgin","title":"Track 4: Nigerian_Pidgin","text":""},{"location":"results/#track-5-amharic","title":"Track 5: Amharic","text":""},{"location":"results/#track-6-algerian-arabic","title":"Track 6: Algerian Arabic","text":""},{"location":"results/#track-7-moroccan-arabicdarija","title":"Track 7: Moroccan Arabic/Darija","text":""},{"location":"results/#track-8-swahili","title":"Track 8: Swahili","text":""},{"location":"results/#track-9-kinyarwanda","title":"Track 9: Kinyarwanda","text":""},{"location":"results/#track-10-twi","title":"Track 10: Twi","text":""},{"location":"results/#track-11-mozambican-portuguese","title":"Track 11: Mozambican Portuguese","text":""},{"location":"results/#track-12-xitsonga-mozambique-dialect","title":"Track 12: Xitsonga (Mozambique Dialect)","text":""},{"location":"results/#track-16-multilingual","title":"Track 16: Multilingual","text":""},{"location":"results/#track-17-zero-shot-on-tigrinya","title":"Track 17: Zero-Shot on Tigrinya","text":""},{"location":"results/#track-18-zero-shot-on-oromo","title":"Track 18: Zero-Shot on Oromo","text":""}]}