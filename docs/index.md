
<center>

#   ** Semantic Sentence Textual Relatedness** 
SemEval 2024 Task 1: Semantic Textual Relatedness Shared Task

> Part of the [18th International Workshop on Semantic Evaluation](https://semeval.github.io/SemEval2024/tasks.html)

<!-- 
<center> -->
> Contact organizers at: [afrisenti-semeval-organizers@googlegroups.com](mailto:afrisenti-semeval-organizers@googlegroups.com)
<!-- 
<center> -->

> Join Task Google Group: [afrisenti-semeval-organizers@googlegroups.com](mailto:afrisenti-semeval-organizers@googlegroups.com)


<!-- <center> -->

> Join Task Slack: [afrisenti-semeval-organizers@googlegroups.com](mailto:afrisenti-semeval-organizers@googlegroups.com)

<!-- 
>  [Visit CodaLab competition website](https://codalab.lisn.upsaclay.fr/competitions/7320) -->



<!-- <font size=3> <span style="color: blue;"> AfriSenti dataset is available at task's:[GitHub repo](https://github.com/afrisenti-semeval/afrisent-semeval-2023) </span> </font>
 -->

</center>


---
## **Motivation**

The semantic relatedness of two language units has long been considered fundamental to understanding meaning (<cite>George,A Miller, 1991</cite>), and automatically determining relatedness has many applications such as evaluating sentence representation methods, question answering, and summarization (<cite>abdalla,2021; wang2023</cite>).

Two sentences are considered semantically similar when they have a paraphrasal or entailment relation (e.g., <span style='color: blue;'>The boy scored the winning goal.</span>, <span style='color: blue;'>The boy scored the winning goal.</span>). On the other hand, relatedness is a much broader concept that accounts for all the commonalities between two sentences. For example  <span style='color: blue;'>The boy scored the winning goal.</span> and <span style='color: blue;'>The boy's team moved onto the finals.</span> would be related though not similar. 

<p>
However, much of past NLP work is on semantic similarity, and largely focused on English. In this shared task, we have covered 17 African languages, <a href="https://en.wikipedia.org/wiki/Hausa_language">Hausa</a>, <a href="https://en.wikipedia.org/wiki/Yoruba_language">Yoruba</a>, <a href="https://en.wikipedia.org/wiki/Igbo_language">Igbo</a>, <a href="https://en.wikipedia.org/wiki/Nigerian_Pidgin">Nigerian </a>Pidgin from Nigeria, <a href="https://en.wikipedia.org/wiki/Amharic, and Oromo">Amharic</a>, <a href="https://en.wikipedia.org/wiki/Tigrinya_language">Tigrinya</a>, and <a href="https://en.wikipedia.org/wiki/Oromo_language">Oromo</a> from Ethiopia, <a href="https://en.wikipedia.org/wiki/Swahili_language">Swahili</a> from Kenya and Tanzania, <a href="https://en.wikipedia.org/wiki/Algerian_Arabic">Algerian Arabic</a> dialect from Algeria, <a href="https://en.wikipedia.org/wiki/Kinyarwanda">Kinyarwanda</a> from Rwanda, <a href="https://en.wikipedia.org/wiki/Twi">Twi</a> from Ghana, <a href="https://www.google.com/search?client=safari&amp;rls=en&amp;q=Mozabique+portuguess&amp;ie=UTF-8&amp;oe=UTF-8">Mozambique Portuguese</a> from Mozambique &nbsp;and <a href="https://en.wikipedia.org/wiki/Moroccan_Arabic">Moroccan Arabic/Darija</a> from Morocco.</p>

## **Task Overview**
<p>The <strong> Semantic Textual Relatedness Shared Task 1</strong> 
is the task of predicting semantic textual relatedness (STR) of sentence pairs. Participants will rank sentence pairs by their closeness in meaning (i.e., their degree of semantic relatedness) \cite{mohammad2008measuring} in 14 languages predominantly from Africa and Asia. All sentence pairs will have manually determined relatedness scores between 0 (completely unrelated) and 1 (maximally related). It consists of two sub-tasks. Participants can select one or more sub-tasks depending on their preference.</p>


<blockquote>
<p><strong>Task A: Monolingual Sentiment Classification</strong></p>
</blockquote>
<p>Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet conveys both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. This sub-task has 15 tracks:</p>

<p><strong>Task B: Monolingual Sentiment Classification</strong></p>
</blockquote>
<p>Given training data in a target language, determine the polarity of a tweet in the target language (positive, negative, or neutral). If a tweet conveys both a positive and negative sentiment, whichever is the stronger sentiment should be chosen. This sub-task has 15 tracks:</p>



## **Important Dates**

| Descriptions                 | Deadlines                                    |
| ---------------------------- | -------------------------------------------- |
| Sample Data Ready            | <s>15 July 2022</s>                          |
| Training Data Ready          | 01 September 2023                            |
| Evaluation Start             | 10 January 2024                              |
| Evaluation End               | 31 January 2024                              |
| System Description Paper Due | 29 February 2024                             |
| Notification to authors      | 1 April 2024                                 |
| Camera ready due             | 22 April 2024                                |
| SemEval workshop 2023        | TBD (co-located with a major NLP conference) |

All deadlines are 23:59 UTC-12 ("anywhere on Earth").


<!-- ## **Communication**

- Join [Task Mailing List](https://groups.google.com/g/afrisenti-semeval)
- Join [Task Slack Channel](https://join.slack.com/t/afrisenti-semeval/shared_invite/zt-1fds98x1u-L3c~bpBI91IWRD80_Fy23Q) to communicate with the organizers.
- Contact Organizers: [afrisenti-semeval-organizers@googlegroups.com](mailto:afrisenti-semeval-organizers@googlegroups.com) -->


## **References**

1. George A Miller and Walter G Charles. 1991. Contex-
tual Correlates of Semantic Similarity. Language
and Cognitive Processes, 6(1):1â€“28




<style>
body {
text-align: justify}
</style>
